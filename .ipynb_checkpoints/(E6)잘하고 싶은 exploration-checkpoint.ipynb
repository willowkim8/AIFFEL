{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominant-directory",
   "metadata": {},
   "source": [
    "GPT-2는 샘 알트만과 일론 머스크가 공동 설립한 오픈AI재단이 개발한 문장 생성 전문 AI  \n",
    "  \n",
    "__시퀀스(sequence)__ : 데이터에 순서(번호)를 붙여 나열  \n",
    "아이스크림, 커피, 설탕, 쿠키, 우유  \n",
    "이 목록은 일종의 시퀀스다. 시퀀스에서는 순서로 요소를 가리킬 수 있다.   \n",
    "이 예처럼 시퀀스는 데이터를 하나씩 순서대로 나열한 것으로, 한 위치를 가리켜 데이터를 집어내는 것도 가능하다.   \n",
    "  \n",
    "**시퀀스의 특징**\n",
    ">데이터를 순서대로 하나씩 나열하여 나타낸 데이터 구조다.  \n",
    " 특정 위치(~번째)의 데이터를 가리킬 수 있다.  \n",
    " \n",
    "  \n",
    "나열과 정렬은 다르다  \n",
    "시퀀스에 담은 데이터의 순서란 데이터의 나열 순서일 뿐이다. 순서가 있다는 것과 정렬되었다는 것은 의미가 다르다. 시퀀스의 데이터는 어떤 기준에 따라 정렬되었을 수도 있고 무작위로 나열되었을 수도 있다. 어쨌든 데이터를 관리하기 위한 순서(번호)는 반드시 존재한다. \n",
    "시퀀스는 그저 나열이다.  \n",
    "  \n",
    "파이썬은 리스트(list), 튜플(tuple), 레인지(range), 문자열(string) 등이 여러 가지 시퀀스 컬렉션을 제공\n",
    "   \n",
    "인공지능이 예측을 하려면 어느 정도는 연관성이 있어줘야 합니다.  \n",
    "문법을 인공지능이 그대로 배워서 문장 데이터를 예측할 리가 만무하니, 좀 더 단순한 접근 방법을 취해야겠죠. 바로 통계에 기반한 방법입니다!  \n",
    "많은 글을 읽게 함으로써 (훈련하므로써) 각 단어 뒤에 올 단어를 예측한다. 즉, 데이터가 많을 수록 결과는 좋아집니다.  RNN(순환 신경망)이 가장 이걸 잘한다.  \n",
    "![](https://aiffelstaticprd.blob.core.windows.net/media/images/E-12-RNN2.max-800x600.png)\n",
    "\\<start> 라는 특수한 토큰을 맨 앞에 추가해서 문장을 시작하라는 사인을 알아들으면 나는 -> 밥을 -> 먹었다 -> \\<end>순으로 순환하다.\\<start> 가 문장의 시작에 더해진 입력 데이터(문제지)와, \\<end> 가 문장의 끝에 더해진 출력 데이터(답안지)가 필요  \n",
    ">source_sentence => X_train  \n",
    "target_sentence => y_train  \n",
    "\n",
    "위와 같이 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "later-creator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 문장: <start> 나는 밥을 먹었다 \n",
      "Target 문장:  나는 밥을 먹었다 <end>\n"
     ]
    }
   ],
   "source": [
    "# 위 과정을 파이썬으로\n",
    "sentence = \" 나는 밥을 먹었다 \"\n",
    "\n",
    "source_sentence = \"<start>\" + sentence\n",
    "target_sentence = sentence + \"<end>\"\n",
    "\n",
    "print(\"Source 문장:\", source_sentence)\n",
    "print(\"Target 문장:\", target_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-composite",
   "metadata": {},
   "source": [
    "### 언어 모델 (Language Model)\n",
    "'나는' 뒤에 올 수 있는 자연스러운 단어의 경우의 수가 워낙 많다 보니 불확실성이 높을 뿐  \n",
    "n-1개의 단어 시퀀스 w_1,,,, w_{n-1}가 주어졌을 때, n번째 단어 w_n으로 무엇이 올지를 예측하는 확률 모델을 언어 모델(Language Model)이라고 부릅니다. 파라미터 \\theta 모델링하는 언어 모델을 다음과 같이 표현  \n",
    "P(w_n | w_1,,,, w_{n-1} ; \\theta )  \n",
    "\n",
    "n-1번째까지의 단어 시퀀스가 x_train이 되고 n번째 단어가 y_train이 되는 데이터셋은 무궁무진하게 만들 수 있다.  \n",
    "이전 스텝에서 소개했던 GPT-2 같은 문장 생성기도 언어 모델의 한 종류에 불과합니다. 딥러닝 모델의 구조나 파라미터 사이즈, 학습데이터의 양 등이 특별할 뿐, 기본적인 원리는 오늘 우리가 만들게 될 언어 모델과 전혀 다를게 없습니다.  \n",
    "\n",
    "## 6-4. 실습 (1) 데이터 다듬기\n",
    "텐서플로우(TensorFlow)가 제공하는 셰익스피어의 연극 대본을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "enhanced-anderson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import os\n",
    "\n",
    "# 파일을 읽기모드로 열어 봅니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()   # 텍스트를 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "\n",
    "print(raw_corpus[:9])    # 앞에서부터 10라인만 화면에 출력해 볼까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-wireless",
   "metadata": {},
   "source": [
    "![](https://aiffelstaticprd.blob.core.windows.net/media/images/E-12-3.data.max-800x600.png)\n",
    ">우린 문장(대사)만을 원하므로 화자 이름이나 공백뿐인 정보는 필요가 없다. -> 1차 필터링\n",
    " 화자가 표기된 문장은 문장의 끝이 :로 끝남 -> : 를 기준으로 문장을 제외  \n",
    " 공백 -> 길이를 검사하여 길이가 0이라면 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cooperative-eleven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-sandwich",
   "metadata": {},
   "source": [
    "텍스트 생성 모델에도 단어 사전을 만들게 됩니다. 그렇다면 문장을 일정한 기준으로 쪼개야겠죠? 그 과정을 토큰화(Tokenize) 라고 합니다. -> 띄어쓰기를 기준으로 나누는 방법 사용, but 약간의 문제 있음\n",
    "> 1. Hi, my name is John. \\*(\"Hi,\" \"my\", …, \"john.\" 으로 분리됨) - 문장부호  \n",
    "2. First, open the first chapter. \\*(First와 first를 다른 단어로 인식) - 대소문자  \n",
    "3. He is a ten-year-old boy. \\*(ten-year-old를 한 단어로 인식) - 특수문자  \n",
    "  \n",
    "문제 해결   \n",
    ">\"1.\" 을 막기 위해 문장 부호 양쪽에 공백을 추가  \n",
    "\"2.\" 를 막기 위해 모든 문자들을 소문자로 변환  \n",
    "\"3.\"을 막기 위해 특수문자들은 모두 제거\n",
    "\n",
    "이런 전처리를 위해 정규표현식(Regex)을 이용한 필터링이 유용하게 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pointed-stage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "charged-theta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus: # \n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-sterling",
   "metadata": {},
   "source": [
    "배우고자 하는 언어 를 모국어로 표현 을 해야 공부를 할 수 있다. 인공지능의 모국어라면 단연 숫자.  \n",
    "tf.keras.preprocessing.text.Tokenizer 패키지는 정제된 데이터를 토큰화하고,  \n",
    "단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며,  \n",
    "데이터를 숫자로 변환까지 한 방에 해줍니다. 이 과정을 벡터화(vectorize) 라 하며,  \n",
    "숫자로 변환된 데이터를 텐서(tensor) 라고 칭합니다.  \n",
    "우리가 사용하는 텐서플로우로 만든 모델의 입출력 데이터는 실제로는 모두 이런 텐서로 변환되어 처리되는 것입니다.  \n",
    "  \n",
    "여기서 **tensor**란 매우 수학적인 개념으로 데이터의 배열  \n",
    "matrix의 집합인 tensor는 당연히 3차원 부터 시작  \n",
    "1. Tensor는 배열의 집합이다.\n",
    "2. 차원의 수는 Rank와 같은말이다.\n",
    "3. 배열의 차원에따라 불리는 이름이 달라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fitting-rugby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]]\n",
      "<keras_preprocessing.text.Tokenizer object at 0x7fc72841ed10>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor, tokenizer, sep='\\n')\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "written-payment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]\n",
      " [   2 1201    5 1201    5    3    0    0    0    0]\n",
      " [   2  199    4   11   92 1021  298   18 2314  513]]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력\n",
    "print(tensor[:5, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conceptual-voluntary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n",
      "11 : you\n",
      "12 : my\n",
      "13 : a\n",
      "14 : that\n",
      "15 : ?\n",
      "16 : in\n",
      "17 : !\n",
      "18 : is\n",
      "19 : not\n",
      "20 : for\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전이 어떻게 구축되었는지 아래와 같이 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 20: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-vatican",
   "metadata": {},
   "source": [
    ">왜 모든 행이 2로 시작하는지 이해할 수 있음 -> 2번 인덱스가 바로 <start>\n",
    "    \n",
    "이제 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "distributed-florence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-postage",
   "metadata": {},
   "source": [
    "> \\<end>가 안 잘리고 <pad>가 잘려서 tensor에 아직 3이 남아있다.\n",
    "    \n",
    "소스는 2(\\<start\\>)에서 시작해서 3(\\<end\\>)으로 끝난 후 0(\\<pad\\>)로 채워져 있습니다. 하지만 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한칸 시프트한 형태를 가지고 있습니다.  \n",
    "    \n",
    "데이터셋 객체를 생성  \n",
    "텐서플로우를 활용할 경우 텐서로 생성된 데이터를 이용해 tf.data.Dataset객체를 생성하는 방법을 흔히 사용\n",
    "tf.data.Dataset객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의기능을 제공하므로 꼭 사용법을 알아 두시기를 권합니다.   \n",
    "이미 데이터셋을 텐서 형태로 생성해 두었으므로, tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset객체를 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "noticed-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(src_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adverse-regression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(type(dataset))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-belize",
   "metadata": {},
   "source": [
    "# 이해안간다고오!!!\n",
    "+ len(src_input) ->\n",
    "src_input 행렬인거 같은데 어떻게 lenth를 잼? 행을 재는건지, 열을 재는건지, 아니면 행렬이 아닌건지   \n",
    "  => 행 개수임.\n",
    "  \n",
    "+ 우리가 만들 모델은 tf.keras.Model을 Subclassing하는 방식으로 만들 것입니다. \n",
    "-> 뭔말이죠...???\n",
    "\n",
    "+ test dataset & label 없이 훈력했다 -> 비지도 학습인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-stanley",
   "metadata": {},
   "source": [
    "정규표현식을 이용한 corpus 생성  \n",
    "tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환  \n",
    "tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환  \n",
    " \n",
    "## 6-5. 실습 (2) 인공지능 학습시키기\n",
    "우리가 만들고자 하는 인공지능을 모델(model)이라고 칭하겠다.  \n",
    "![](https://aiffelstaticprd.blob.core.windows.net/media/images/E-12-4.max-800x600.png)\n",
    "우리가 만들 모델에는 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "australian-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) # Embedding 레이어\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-stevens",
   "metadata": {},
   "source": [
    "embedding_size 는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기  \n",
    "만약 그 크기가 2라면 예를 들어  \n",
    "차갑다: \\[0.0, 1.0]  \n",
    "뜨겁다: \\[1.0, 0.0]  \n",
    "미지근하다: \\[0.5, 0.5]  \n",
    "이번 실습에서는 256이 적당해 보임  \n",
    "***\n",
    "LSTM 레이어의 hidden state 의 차원수인 hidden_size 도 같은 맥락  \n",
    "hidden_size 는 모델에 얼마나 많은 일꾼을 둘 것인가?   \n",
    "이번 실습에는 1024가 적당해보임  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "romantic-forwarding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-1.33035122e-04,  3.10441304e-04,  1.60072334e-04, ...,\n",
       "          1.27030260e-04, -1.35130649e-05, -1.08103384e-04],\n",
       "        [ 7.67267993e-05,  4.49882587e-04,  2.51672202e-04, ...,\n",
       "          4.04916500e-04, -4.81538154e-05, -1.80670075e-04],\n",
       "        [ 1.91817904e-04,  6.08158007e-04,  6.54666044e-04, ...,\n",
       "          5.79614309e-04,  8.60485889e-05, -1.83865530e-04],\n",
       "        ...,\n",
       "        [-3.03422962e-03, -1.33842230e-03, -6.28186899e-05, ...,\n",
       "         -7.43316836e-04, -1.07813196e-03,  9.65379702e-04],\n",
       "        [-3.42814880e-03, -1.38091925e-03,  6.56877819e-05, ...,\n",
       "         -9.31465707e-04, -1.14505610e-03,  7.32824672e-04],\n",
       "        [-3.77917918e-03, -1.40653341e-03,  1.76569694e-04, ...,\n",
       "         -1.04537082e-03, -1.15330284e-03,  4.83880925e-04]],\n",
       "\n",
       "       [[-1.33035122e-04,  3.10441304e-04,  1.60072334e-04, ...,\n",
       "          1.27030260e-04, -1.35130649e-05, -1.08103384e-04],\n",
       "        [-2.37458677e-04,  3.22117296e-04,  3.83923150e-04, ...,\n",
       "          3.37827019e-04, -8.11870850e-05, -3.06601403e-04],\n",
       "        [-5.51647041e-04, -7.41463082e-05,  4.68415936e-04, ...,\n",
       "          4.88299876e-04,  2.18795449e-05, -6.06967020e-04],\n",
       "        ...,\n",
       "        [-4.58586030e-03, -1.26048061e-03,  7.58354843e-04, ...,\n",
       "         -1.76198350e-03, -8.01704242e-04, -3.35321791e-04],\n",
       "        [-4.79222508e-03, -1.29155279e-03,  7.71542836e-04, ...,\n",
       "         -1.65122503e-03, -7.11998087e-04, -5.06246521e-04],\n",
       "        [-4.95531596e-03, -1.32104428e-03,  7.69585604e-04, ...,\n",
       "         -1.52232754e-03, -6.08217379e-04, -6.71288173e-04]],\n",
       "\n",
       "       [[-1.33035122e-04,  3.10441304e-04,  1.60072334e-04, ...,\n",
       "          1.27030260e-04, -1.35130649e-05, -1.08103384e-04],\n",
       "        [-3.99680430e-04,  3.62562831e-04,  5.81568311e-05, ...,\n",
       "         -8.17802429e-05, -2.52299011e-04,  1.28170883e-04],\n",
       "        [-6.37814403e-04,  4.63845616e-04,  9.24658380e-05, ...,\n",
       "         -3.82829130e-05, -3.97951837e-04,  6.32712094e-04],\n",
       "        ...,\n",
       "        [-3.69002228e-03, -8.84219597e-04,  3.33923817e-04, ...,\n",
       "         -2.44982820e-03, -8.75633676e-04,  1.85241908e-04],\n",
       "        [-4.03047726e-03, -1.03516644e-03,  3.47342982e-04, ...,\n",
       "         -2.48665828e-03, -8.75410449e-04,  9.91053894e-05],\n",
       "        [-4.32301126e-03, -1.15335756e-03,  3.59721802e-04, ...,\n",
       "         -2.44157715e-03, -8.37957428e-04, -2.19864942e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.33035122e-04,  3.10441304e-04,  1.60072334e-04, ...,\n",
       "          1.27030260e-04, -1.35130649e-05, -1.08103384e-04],\n",
       "        [-1.44937760e-04,  5.15697175e-04,  3.87238862e-04, ...,\n",
       "         -1.53179193e-04,  3.24320172e-05, -1.17089090e-04],\n",
       "        [-3.61171435e-04,  4.55092930e-04,  6.75110379e-04, ...,\n",
       "         -3.56042583e-04,  1.18299926e-04, -2.73807294e-04],\n",
       "        ...,\n",
       "        [-4.53896308e-03, -1.56548305e-03,  9.25946049e-04, ...,\n",
       "         -2.11368385e-03, -5.91725693e-04, -6.26601279e-04],\n",
       "        [-4.75487253e-03, -1.58544537e-03,  8.98314349e-04, ...,\n",
       "         -1.97131769e-03, -5.16759756e-04, -7.63059186e-04],\n",
       "        [-4.92537720e-03, -1.60025142e-03,  8.62992951e-04, ...,\n",
       "         -1.80749048e-03, -4.26136336e-04, -8.97719758e-04]],\n",
       "\n",
       "       [[-1.33035122e-04,  3.10441304e-04,  1.60072334e-04, ...,\n",
       "          1.27030260e-04, -1.35130649e-05, -1.08103384e-04],\n",
       "        [-4.07658226e-04,  7.82527146e-04,  1.90204228e-04, ...,\n",
       "          8.48124182e-05, -1.86587378e-04, -2.67735217e-04],\n",
       "        [-2.83177302e-04,  1.05321035e-03,  4.22117118e-05, ...,\n",
       "         -1.55332993e-04, -2.07175908e-04, -5.19164896e-04],\n",
       "        ...,\n",
       "        [-3.14587587e-03, -4.90402046e-04,  1.68618571e-03, ...,\n",
       "         -1.66829838e-03, -9.83239966e-04, -6.52073417e-04],\n",
       "        [-3.57178203e-03, -6.83802064e-04,  1.60366332e-03, ...,\n",
       "         -1.75376481e-03, -9.04242566e-04, -7.21769524e-04],\n",
       "        [-3.93567421e-03, -8.53021047e-04,  1.50582136e-03, ...,\n",
       "         -1.77241489e-03, -8.07986478e-04, -8.05043557e-04]],\n",
       "\n",
       "       [[-1.33035122e-04,  3.10441304e-04,  1.60072334e-04, ...,\n",
       "          1.27030260e-04, -1.35130649e-05, -1.08103384e-04],\n",
       "        [-8.93278338e-05,  6.46682747e-04,  4.23534511e-04, ...,\n",
       "         -5.75439881e-05, -7.73017746e-05,  7.88786056e-05],\n",
       "        [-1.54035006e-04,  6.52529649e-04,  7.75419874e-04, ...,\n",
       "          2.96346607e-05,  9.20249440e-05, -2.07780133e-04],\n",
       "        ...,\n",
       "        [-3.73149314e-03, -1.61833351e-03,  2.48743221e-04, ...,\n",
       "         -1.36717432e-03, -7.35994894e-04,  6.24511624e-04],\n",
       "        [-4.03641677e-03, -1.62977492e-03,  3.67343862e-04, ...,\n",
       "         -1.45900203e-03, -8.45371920e-04,  5.26763964e-04],\n",
       "        [-4.30381810e-03, -1.63036631e-03,  4.61878401e-04, ...,\n",
       "         -1.49307144e-03, -8.99544859e-04,  3.85046675e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델의 최종 출력 텐서 shape\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "print(model(src_sample))\n",
    "print(model(tgt_sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-oakland",
   "metadata": {},
   "source": [
    "> shape=(256, 20, 7001)\n",
    "- 256 : 이전 스텝에서 지정한 배치 사이즈\n",
    "- 20 : tf.keras.layers.LSTM(hidden_size, return_sequences=True)로 호출한 LSTM 레이어에서 return_sequences=True이라고 지정한 부분에 있습니다. 즉, LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미입니다. 만약 return_sequences=False였다면 LSTM 레이어는 1개의 벡터만 출력했을 것입니다. 그런데 문제는, 우리의 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모른다는 점입니다. 모델을 만들면서 알려준 적도 없습니다. 그럼 20은 언제 알게된 것일까요? 네, 그렇습니다.   \n",
    "데이터를 입력받으면서 비로소 알게 된 것입니다. 우리 데이터셋의 max_len이 20으로 맞춰져 있었던 것입니다.  \n",
    "- 7001 : Dense 레이어의 출력 차원수입니다. 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링해야 하기 때문입니다. VOCAB_SIZE = tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "massive-breathing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-worship",
   "metadata": {},
   "source": [
    "그동안 많이 보았던 것과는 다른 점이 있습니다. 우리가 궁금했던 Output Shape를 정확하게 알려주지 않습니다.  \n",
    "우리의 모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "characteristic-strand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 13s 143ms/step - loss: 3.4946\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 13s 144ms/step - loss: 2.8087\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 13s 141ms/step - loss: 2.7093\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 13s 142ms/step - loss: 2.6071\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 13s 142ms/step - loss: 2.5440\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 13s 144ms/step - loss: 2.4930\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 2.4319\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 13s 144ms/step - loss: 2.3771\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 13s 143ms/step - loss: 2.3267\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 13s 144ms/step - loss: 2.2770\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 13s 144ms/step - loss: 2.2292\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 2.1842\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 2.1380\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 2.0928\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 2.0468\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 13s 144ms/step - loss: 2.0021\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 1.9554\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 14s 145ms/step - loss: 1.9110\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 13s 143ms/step - loss: 1.8625\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 14s 145ms/step - loss: 1.8167\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 1.7702\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 14s 145ms/step - loss: 1.7232\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 14s 145ms/step - loss: 1.6753\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 1.6302\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 1.5837\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 1.5345\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 1.4879\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 1.4411\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 13s 143ms/step - loss: 1.3930\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 13s 145ms/step - loss: 1.3424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc78f213c90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-candy",
   "metadata": {},
   "source": [
    "## 6-6. 실습 (3) 잘 만들어졌는지 평가하기\n",
    "모델이 작문을 잘하는지 컴퓨터 알고리즘이 평가하는 것은 무리  \n",
    "-> 따라서 작문 모델을 평가하는 가장 확실한 방법은 작문을 시켜보고 직접 평가하는 것  \n",
    "generate_text 함수 : 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "correct-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True: # 주목!\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-mixture",
   "metadata": {},
   "source": [
    "학습 단계에서 우리는 이런 while 문이 필요없었습니다.  \n",
    "소스 문장을 모델에 입력해서 나온 결과를 타겟 문장과 직접 비교하면 그만이었기 때문에  \n",
    "  \n",
    "그러나 텍스트를 실제로 생성해야 하는 시점에서 우리는 타겟 문장과 소스 문장이 없다. 생각해 보면 우리는 텍스트 생성 태스크를 위해 테스트 데이터셋을 따로 생성한 적이 없습니다. => 비지도?인가요?  \n",
    "  \n",
    "generate_text() 함수에서 init_sentence를 인자로 받고는 있습니다. 이렇게 받은 인자를 일단 텐서로 만들고 있습니다. 디폴트로는 \\<start> 단어 하나만 받음.\n",
    "  \n",
    "**순서(with RNN)**  \n",
    "1.while의 첫번째 루프에서 test_tensor에 \\<start> 하나만 들어갔다고 합시다. 우리의 모델이 출력으로 7001개의 단어 중 A를 골랐다고 합시다  \n",
    "2.while의 두번째 루프에서 test_tensor에는 \\<start> A가 들어갑니다. 그래서 우리의 모델이 그다음 B를 골랐다고 합시다.  \n",
    "3.while의 세번째 루프에서 test_tensor에는 \\<start> A B가 들어갑니다.  \n",
    "4.그래서….. (이하 후략)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abroad-merit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> the <unk> of the smallest spider s web , <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init_sentence 를 바꿔가며 해보기 \n",
    "generate_text(model, tokenizer, init_sentence=\"<start> the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-vietnam",
   "metadata": {},
   "source": [
    "## 6-7. 프로젝트: 멋진 작사가 만들기\n",
    "1) Step 1. 데이터 다운로드  \n",
    "Song Lyrics 데이터를 다운  \n",
    "  \n",
    "2) Step 2. 데이터 읽어오기  \n",
    "  \n",
    "3) Step 3. 데이터 정제  \n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!  \n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.  \n",
    "  \n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래가사 작사하기에 어울리지 않을수도 있겠죠.\n",
    "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기를 권합니다.  \n",
    "  => 토큰화 전에 단어 개수가 13개인 \n",
    "  \n",
    "  \n",
    "4) Step 4. 평가 데이터셋 분리  \n",
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20%를 평가 데이터셋으로 사용해 주세요!  \n",
    "```\n",
    "enc_train, enc_val, dec_train, dec_val = <코드 작성>\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "\n",
    "\n",
    "out:\n",
    "\n",
    "Source Train: (124960, 14)\n",
    "Target Train: (124960, 14)\n",
    "```\n",
    "\n",
    "  \n",
    "5) Step 5. 인공지능 만들기  \n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)  \n",
    "``` \n",
    "#Loss\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    \n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-romance",
   "metadata": {},
   "source": [
    "1) Step 1. 데이터 다운로드  \n",
    "Song Lyrics 데이터를 다운  \n",
    "  \n",
    "2) Step 2. 데이터 읽어오기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "religious-reservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 2400\n",
      "Examples:\n",
      " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
     ]
    }
   ],
   "source": [
    "import glob # 파일을 읽어오는 작업을 하기가 아주 용이\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/adele.txt'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-sleeping",
   "metadata": {},
   "source": [
    "3) Step 3. 데이터 정제  \n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!  \n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-soviet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "serious-panel",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-masters",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-student",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-horror",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-launch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-rubber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-profit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-scheme",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
