{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tribal-rental",
   "metadata": {},
   "source": [
    "# (E6) Song_lyrics_project_InYu\n",
    "## 1) Step 1. 데이터 다운로드  \n",
    "Song Lyrics 데이터를 다운  \n",
    "  \n",
    "## 2) Step 2. 데이터 읽어오기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incomplete-monday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['How does a bastard, orphan, son of a whore', 'And a Scotsman, dropped in the middle of a forgotten spot in the Caribbean by providence impoverished,', 'In squalor, grow up to be a hero and a scholar? The ten-dollar founding father without a father']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tensorflow as tf  \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import glob # 파일을 읽어오는 작업을 하기가 아주 용이\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*' # * = all : 모두 읽는다.\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-fifty",
   "metadata": {},
   "source": [
    "## 3) Step 3. 데이터 정제  \n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!  \n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "answering-dependence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mechanical-microwave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> how does a bastard , orphan , son of a whore <end>',\n",
       " '<start> and a scotsman , dropped in the middle of a forgotten spot in the caribbean by providence impoverished , <end>',\n",
       " '<start> in squalor , grow up to be a hero and a scholar ? the ten dollar founding father without a father <end>',\n",
       " '<start> got a lot farther by working a lot harder <end>',\n",
       " '<start> by being a lot smarter by being a self starter <end>',\n",
       " '<start> by fourteen , they placed him in charge of a trading charter and every day while slaves were being slaughtered and carted away <end>',\n",
       " '<start> across the waves , he struggled and kept his guard up <end>',\n",
       " '<start> inside , he was longing for something to be a part of <end>',\n",
       " '<start> the brother was ready to beg , steal , borrow , or barter then a hurricane came , and devastation reigned <end>',\n",
       " '<start> our man saw his future drip , dripping down the drain <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus: # \n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-satisfaction",
   "metadata": {},
   "source": [
    "**문장 토큰 개수를 12개로 조정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nominated-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2    78   626 ...     3     0     0]\n",
      " [   14     6   881 ... 11331     4     3]\n",
      " [    9  2040     8 ...     9   635     3]\n",
      " ...\n",
      " [    2   207     1 ...     0     0     0]\n",
      " [    2    76     7 ...     0     0     0]\n",
      " [    2    14     6 ...     0     0     0]]\n",
      "<keras_preprocessing.text.Tokenizer object at 0x7f49b79bf210>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 15)  \n",
    "\n",
    "    print(tensor, tokenizer, sep='\\n')\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acting-newport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2    78   626     9  4376     4  5629     4   611    19]\n",
      " [   14     6   881    19     9  1974   808    14     6  4179]\n",
      " [    9  2040     8     9  7655    43     6   743  1128 11332]\n",
      " [    2    41     9   396  3196   122  1121     9   396  1032]\n",
      " [    2   122   543     9   396  7656   122   543     9  1202]]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력\n",
    "print(tensor[:5, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-young",
   "metadata": {},
   "source": [
    "# \\[1]번 보니까 앞에 부분을 잘라서 길이 맞췄다고 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "romance-essay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : t\n",
      "16 : s\n",
      "17 : that\n",
      "18 : on\n",
      "19 : of\n",
      "20 : .\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전이 어떻게 구축되었는지 아래와 같이 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 20: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-behalf",
   "metadata": {},
   "source": [
    "## 4) Step 4. 평가 데이터셋 분리  \n",
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20%를 평가 데이터셋으로 사용해 주세요!   \n",
    "  \n",
    "```\n",
    "enc_train, enc_val, dec_train, dec_val = <코드 작성>\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "\n",
    "\n",
    "out:\n",
    "\n",
    "Source Train: (124960, 14)\n",
    "Target Train: (124960, 14)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "formed-enemy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   78  626    9 4376    4 5629    4  611   19    9 3293    3    0]\n",
      "[  78  626    9 4376    4 5629    4  611   19    9 3293    3    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게\n",
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "removed-briefing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(type(dataset))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "working-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input, \n",
    "                                                          test_size=0.2, \n",
    "                                                          random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "animated-mileage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-offense",
   "metadata": {},
   "source": [
    "## 5) Step 5. 인공지능 만들기  \n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)  \n",
    "``` \n",
    "#Loss\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    \n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coupled-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) # Embedding 레이어\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "grave-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-3.52564035e-04  1.26818166e-04  2.26878401e-05 ... -1.59725590e-04\n",
      "   -2.96855324e-05  4.03979684e-05]\n",
      "  [-6.47152774e-04  1.92158855e-04 -2.40566995e-04 ... -2.92696583e-04\n",
      "    5.54590551e-05 -1.33412486e-05]\n",
      "  [-7.82305957e-04  5.33920538e-04 -4.47150756e-04 ... -2.64935545e-04\n",
      "    7.59503382e-05  2.52410013e-04]\n",
      "  ...\n",
      "  [ 8.39077751e-04  3.73434945e-04 -1.65296521e-03 ...  5.47785719e-04\n",
      "    2.32313796e-05  6.09501498e-04]\n",
      "  [ 9.57746641e-04  4.44326899e-04 -1.35485071e-03 ...  3.14443139e-04\n",
      "   -1.85195269e-04  4.86480858e-04]\n",
      "  [ 9.32216062e-04  4.42115794e-04 -1.24084658e-03 ...  3.52790172e-04\n",
      "   -4.21221484e-04  1.82750693e-04]]\n",
      "\n",
      " [[-1.26258121e-04  1.32090034e-04  2.91876611e-04 ...  7.17201692e-05\n",
      "   -7.34588466e-05  6.06088943e-05]\n",
      "  [-8.53275997e-05  1.26052371e-04  5.47657255e-04 ... -1.77632737e-05\n",
      "   -2.98647810e-05  2.49913341e-04]\n",
      "  [-1.89741317e-04  9.09769806e-05  4.95859073e-04 ...  2.18754838e-04\n",
      "   -3.98071497e-05  3.90600035e-05]\n",
      "  ...\n",
      "  [ 3.99190554e-04 -3.66660461e-05 -1.74874040e-05 ... -1.31652760e-03\n",
      "   -1.43504396e-04 -2.05629412e-03]\n",
      "  [ 2.94343306e-04 -8.15174062e-05 -1.06008660e-08 ... -1.53061922e-03\n",
      "   -3.52951058e-04 -2.31198571e-03]\n",
      "  [ 1.87614540e-04 -1.85983663e-04  1.45841705e-05 ... -1.70513545e-03\n",
      "   -5.21404552e-04 -2.51641427e-03]]\n",
      "\n",
      " [[-1.26258121e-04  1.32090034e-04  2.91876611e-04 ...  7.17201692e-05\n",
      "   -7.34588466e-05  6.06088943e-05]\n",
      "  [-1.65923600e-04  6.82476530e-05  5.76877268e-04 ...  1.88427905e-04\n",
      "   -9.12345058e-05  1.20992619e-04]\n",
      "  [-2.05832985e-04  7.56201334e-05  5.83303685e-04 ...  2.90965312e-04\n",
      "    2.31194252e-04  1.97293011e-05]\n",
      "  ...\n",
      "  [-7.44719640e-04 -1.53654400e-04  1.77234397e-04 ...  1.31153676e-04\n",
      "    1.43559370e-03 -4.27110237e-04]\n",
      "  [-5.22663468e-04 -3.86855245e-04  3.59780825e-04 ... -2.78261374e-04\n",
      "    1.62898039e-03 -4.61377291e-04]\n",
      "  [-2.80090346e-04 -4.20527300e-04  4.45239799e-04 ... -6.41571067e-04\n",
      "    1.44876109e-03 -6.47118315e-04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.26258121e-04  1.32090034e-04  2.91876611e-04 ...  7.17201692e-05\n",
      "   -7.34588466e-05  6.06088943e-05]\n",
      "  [-2.76680425e-04  4.80832532e-04  3.02351633e-04 ... -1.04995808e-04\n",
      "   -1.41358003e-04  3.48701229e-04]\n",
      "  [-1.50301916e-04  6.70487701e-04  7.40530668e-04 ... -4.05327301e-04\n",
      "   -2.77704065e-04  3.22876120e-04]\n",
      "  ...\n",
      "  [ 6.70435693e-05  2.25667991e-05  2.57617154e-04 ... -1.41978636e-03\n",
      "    1.69777777e-04 -1.69130985e-03]\n",
      "  [ 1.03985796e-04  6.38284291e-06  2.32049715e-04 ... -1.57769118e-03\n",
      "   -8.73060853e-05 -1.86984823e-03]\n",
      "  [ 9.35523713e-05 -4.10980392e-05  2.16228887e-04 ... -1.72250124e-03\n",
      "   -3.25332512e-04 -2.03548558e-03]]\n",
      "\n",
      " [[-1.26258121e-04  1.32090034e-04  2.91876611e-04 ...  7.17201692e-05\n",
      "   -7.34588466e-05  6.06088943e-05]\n",
      "  [-2.70709017e-04  2.56246101e-04  5.68241172e-04 ... -2.20705318e-04\n",
      "   -1.77999740e-04  2.05933771e-04]\n",
      "  [-4.96381486e-04  3.88340472e-04  5.42967464e-04 ... -2.57530482e-04\n",
      "   -1.32955509e-04  2.90901866e-04]\n",
      "  ...\n",
      "  [ 6.62683597e-05  9.01814317e-04 -8.87170827e-05 ... -9.99548472e-04\n",
      "   -1.23549937e-04  5.81831446e-05]\n",
      "  [ 2.08566911e-04  8.96170794e-04 -1.23595033e-04 ... -1.23084325e-03\n",
      "   -3.74535710e-04 -2.99803709e-04]\n",
      "  [ 2.73415615e-04  8.40667228e-04 -1.33967798e-04 ... -1.43558462e-03\n",
      "   -6.34355762e-04 -6.81470148e-04]]\n",
      "\n",
      " [[-1.26258121e-04  1.32090034e-04  2.91876611e-04 ...  7.17201692e-05\n",
      "   -7.34588466e-05  6.06088943e-05]\n",
      "  [-1.92608524e-04  3.69253918e-04  9.71629997e-05 ...  9.01796593e-05\n",
      "   -9.47415319e-05  7.86339806e-05]\n",
      "  [-4.19015967e-04  8.12345825e-04  2.47759308e-04 ...  9.85830047e-05\n",
      "   -5.18548150e-05  2.38300883e-04]\n",
      "  ...\n",
      "  [ 4.19875942e-05 -1.08858512e-04  4.45050282e-05 ... -1.83217623e-03\n",
      "   -2.85411195e-04 -1.25021348e-03]\n",
      "  [ 1.91986564e-05 -2.40463763e-04  3.18600323e-05 ... -1.98753714e-03\n",
      "   -4.86508041e-04 -1.51460711e-03]\n",
      "  [-7.39246889e-06 -3.97064025e-04  1.67343605e-05 ... -2.10339297e-03\n",
      "   -6.37953985e-04 -1.75248936e-03]]], shape=(256, 14, 12001), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 모델의 최종 출력 텐서 shape\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "print(model(src_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quantitative-governor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lovely-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 106s 154ms/step - loss: 3.6161 - val_loss: 3.2345\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 102s 148ms/step - loss: 3.1432 - val_loss: 2.9958\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 108s 158ms/step - loss: 2.9490 - val_loss: 2.8228\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 113s 165ms/step - loss: 2.7971 - val_loss: 2.6809\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 112s 163ms/step - loss: 2.6682 - val_loss: 2.5535\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 107s 156ms/step - loss: 2.5492 - val_loss: 2.4401\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 109s 158ms/step - loss: 2.4403 - val_loss: 2.3333\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 104s 152ms/step - loss: 2.3393 - val_loss: 2.2322\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 105s 153ms/step - loss: 2.2443 - val_loss: 2.1376\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 109s 160ms/step - loss: 2.1538 - val_loss: 2.0484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f491875e6d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hungarian-postcard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.losses.SparseCategoricalCrossentropy object at 0x7f49187575d0>\n"
     ]
    }
   ],
   "source": [
    "#Loss\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "characteristic-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-forward",
   "metadata": {},
   "source": [
    "## 6) 잘 만들어졌는지 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "imported-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True: # 주목!\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mental-juvenile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> you re the only one that i ever had <end> \n",
      "<start> we re gonna make it <end> \n",
      "<start> hate the way you do it <end> \n",
      "<start> baby , baby , baby , baby , baby , baby , baby <end> \n",
      "<start> i m gonna be the one <end> \n"
     ]
    }
   ],
   "source": [
    "# init_sentence 를 바꿔가며 해보기 \n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> you\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> we\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> hate\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> baby\"))\n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-gregory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-ivory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "color-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) # Embedding 레이어\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model_2 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "optional-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-2.88844807e-04  9.70458277e-05  6.29556889e-05 ... -1.97669229e-04\n",
      "   -1.86969541e-04  2.41660600e-04]\n",
      "  [-1.66058293e-04  5.49879260e-05  1.40596341e-04 ... -3.50759597e-04\n",
      "   -2.04225260e-04  4.38911869e-04]\n",
      "  [-6.56778575e-05 -1.07387670e-04  3.05269758e-04 ... -4.46507009e-04\n",
      "   -1.92103340e-04  4.85905621e-04]\n",
      "  ...\n",
      "  [ 6.54429721e-04 -1.08493702e-03  1.61062926e-03 ... -7.28437153e-04\n",
      "   -1.55786998e-04 -3.50736023e-04]\n",
      "  [ 7.79131660e-04 -1.22236228e-03  1.88533007e-03 ... -9.72622947e-04\n",
      "   -3.79109144e-04 -4.62714641e-04]\n",
      "  [ 1.05757697e-03 -1.54708407e-03  2.08368734e-03 ... -7.93866697e-04\n",
      "   -4.94035776e-04 -6.82351587e-04]]\n",
      "\n",
      " [[ 3.95828174e-05  6.34407043e-05 -3.62718456e-05 ... -3.88473200e-05\n",
      "   -7.90046761e-05  3.09970317e-04]\n",
      "  [-1.93111191e-04 -1.60879194e-04 -8.80693187e-06 ... -1.36169459e-04\n",
      "    1.19207289e-04  3.63930943e-04]\n",
      "  [-3.07751878e-04 -3.54875170e-04  1.97003232e-04 ... -2.43655159e-04\n",
      "    4.97667570e-05  2.25254378e-04]\n",
      "  ...\n",
      "  [-6.97161595e-04 -1.12842547e-03  7.35019974e-04 ...  6.91471680e-04\n",
      "    1.06472580e-03 -9.01540974e-04]\n",
      "  [-3.16232268e-04 -9.40614671e-04  9.20189777e-04 ...  5.54374536e-04\n",
      "    1.13165623e-03 -4.32929548e-04]\n",
      "  [-2.98291052e-05 -6.33244636e-04  1.13256765e-03 ...  3.43281048e-04\n",
      "    1.10320735e-03  2.56029132e-04]]\n",
      "\n",
      " [[ 3.95828174e-05  6.34407043e-05 -3.62718456e-05 ... -3.88473200e-05\n",
      "   -7.90046761e-05  3.09970317e-04]\n",
      "  [ 3.61212587e-05  1.91930783e-04 -1.41909826e-04 ...  9.37763325e-05\n",
      "   -1.75761656e-04  2.00202758e-05]\n",
      "  [ 4.38319912e-05  3.45438108e-04 -3.76906028e-05 ...  3.91917536e-04\n",
      "   -2.18884496e-04 -4.21440636e-04]\n",
      "  ...\n",
      "  [ 9.20189777e-04  9.82514233e-04  3.27607850e-04 ...  1.56229653e-04\n",
      "   -6.40657672e-04  1.63407077e-03]\n",
      "  [ 7.92268780e-04  1.25571270e-03  6.71269023e-04 ... -1.79591312e-04\n",
      "   -5.98271785e-04  2.23260373e-03]\n",
      "  [ 6.39945210e-04  1.47993083e-03  1.03479729e-03 ... -4.85826138e-04\n",
      "   -5.75732091e-04  2.83611356e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3.95828174e-05  6.34407043e-05 -3.62718456e-05 ... -3.88473200e-05\n",
      "   -7.90046761e-05  3.09970317e-04]\n",
      "  [ 3.95464944e-04 -9.72373964e-05  9.85828519e-05 ...  7.12475958e-06\n",
      "   -2.97015824e-04  2.37431217e-04]\n",
      "  [ 6.43588428e-04 -1.27154752e-04  9.92495625e-05 ...  2.79825792e-04\n",
      "   -5.39800618e-04 -7.95950982e-05]\n",
      "  ...\n",
      "  [ 5.77024708e-04  5.04749303e-04  9.48012283e-04 ... -8.97451770e-04\n",
      "   -3.12793360e-04  2.59145093e-03]\n",
      "  [ 6.07776106e-04  8.06032447e-04  1.20201381e-03 ... -1.03213347e-03\n",
      "   -3.09443159e-04  3.24032805e-03]\n",
      "  [ 5.89672127e-04  1.05961785e-03  1.46918162e-03 ... -1.17017550e-03\n",
      "   -3.30375333e-04  3.84125812e-03]]\n",
      "\n",
      " [[ 3.95828174e-05  6.34407043e-05 -3.62718456e-05 ... -3.88473200e-05\n",
      "   -7.90046761e-05  3.09970317e-04]\n",
      "  [-7.51609332e-05  2.64629780e-04 -1.73242399e-04 ... -1.84755845e-04\n",
      "   -2.74961669e-04  2.85424176e-04]\n",
      "  [ 1.56281865e-04  3.25198984e-04 -4.80714312e-04 ... -1.30320172e-04\n",
      "   -2.36443680e-04  1.14472001e-04]\n",
      "  ...\n",
      "  [ 1.04059780e-03  9.00492130e-04  4.41216689e-04 ... -9.20999446e-05\n",
      "   -5.57041494e-04  1.50982232e-03]\n",
      "  [ 9.11955081e-04  1.05033233e-03  7.86032353e-04 ... -2.52786151e-04\n",
      "   -5.09483099e-04  2.20279931e-03]\n",
      "  [ 7.61646836e-04  1.18748716e-03  1.13205984e-03 ... -4.40770236e-04\n",
      "   -4.78889735e-04  2.86407047e-03]]\n",
      "\n",
      " [[-1.36774088e-05  7.95431333e-05  1.20533055e-04 ...  1.05604384e-04\n",
      "   -1.78767979e-04  1.03309125e-04]\n",
      "  [-1.14762923e-04 -7.37341106e-05  1.68895174e-04 ... -2.13883922e-07\n",
      "   -4.16336698e-05  1.01788683e-04]\n",
      "  [-2.87304632e-04 -1.66573649e-04  3.50530201e-04 ...  4.36258706e-05\n",
      "    1.30350352e-04  8.87394854e-05]\n",
      "  ...\n",
      "  [-1.52050855e-03  3.37370075e-05  2.52483675e-04 ... -3.49946095e-05\n",
      "    1.16655319e-04 -8.97688849e-04]\n",
      "  [-1.27228943e-03  7.54193607e-05  7.53196291e-05 ... -1.28906060e-04\n",
      "    3.76371609e-04 -6.12822711e-04]\n",
      "  [-7.86703837e-04 -7.47753220e-05 -1.35732582e-04 ... -2.56085295e-05\n",
      "    4.61069983e-04 -2.45268398e-04]]], shape=(256, 14, 12001), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 모델의 최종 출력 텐서 shape\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "print(model_2(src_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cardiovascular-litigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  18882560  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 80,107,489\n",
      "Trainable params: 80,107,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "serious-irrigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 298s 434ms/step - loss: 3.4644 - val_loss: 2.9731\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 307s 447ms/step - loss: 2.8384 - val_loss: 2.5375\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 305s 444ms/step - loss: 2.4689 - val_loss: 2.1516\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 297s 432ms/step - loss: 2.1299 - val_loss: 1.8248\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 302s 441ms/step - loss: 1.8299 - val_loss: 1.5507\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 308s 449ms/step - loss: 1.5762 - val_loss: 1.3370\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 307s 448ms/step - loss: 1.3710 - val_loss: 1.1732\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 318s 464ms/step - loss: 1.2114 - val_loss: 1.0541\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 314s 457ms/step - loss: 1.0956 - val_loss: 0.9791\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 311s 453ms/step - loss: 1.0215 - val_loss: 0.9308\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "model_2.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none'), \n",
    "                     optimizer=optimizer)                    \n",
    "\n",
    "history = model_2.fit(dataset, epochs=10, validation_data=(enc_val, dec_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-condition",
   "metadata": {},
   "source": [
    "## 6) 잘 만들어졌는지 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "complex-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_2(model_2, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True: # 주목!\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "conventional-promotion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> you keep keep baseball baseball baseball pum pum pum annual annual hau hau cop \n",
      "<start> we keep keep keep grabs grabs grabs roots pum flickin careless flickin flickin careless \n",
      "<start> hate unspoken logic conversation end end bears bears remy remy grady grady determined recognition \n",
      "<start> baby sleep reparations gyal separated shorties shorties shorties swaggy swaggy prophet mayonnaise mayonnaise mayonnaise \n",
      "<start> sleep edward pricks pricks pricks pricks pricks pricks whores hour hour servin servin hour \n"
     ]
    }
   ],
   "source": [
    "# init_sentence 를 바꿔가며 해보기 \n",
    "print(generate_text_2(model_2, tokenizer, init_sentence=\"<start> you\"))\n",
    "\n",
    "print(generate_text_2(model_2, tokenizer, init_sentence=\"<start> we\"))\n",
    "\n",
    "print(generate_text_2(model_2, tokenizer, init_sentence=\"<start> hate\"))\n",
    "\n",
    "print(generate_text_2(model_2, tokenizer, init_sentence=\"<start> baby\"))\n",
    "\n",
    "print(generate_text_2(model_2, tokenizer, init_sentence=\"<start> \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-basket",
   "metadata": {},
   "source": [
    "## 루브릭\n",
    "\n",
    "평가문항\n",
    "1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?  \n",
    "   텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?  \n",
    "   ->hidden size = 1024 일 때는 자연스러운 문장을 출력했지만, 2배로 올리면 전혀 자연스럽지 않은 문장이 나왔다.  \n",
    "   \n",
    "   \n",
    "2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?  \n",
    "   특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?  \n",
    "   -> 노드와 다르게 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외해야했다. 그래서 토큰화 과정에 'maxlen = 15'을 추가해 주었다.\n",
    "  \n",
    "  \n",
    "3. 텍스트 생성모델이 안정적으로 학습되었는가?   \n",
    "   텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?\n",
    "   -> 마지막 loss가 0.9308 으로 아주 낮게 나왔다.  \n",
    "  \n",
    "***  \n",
    "## 고찰  \n",
    "노드에서 적절한 비유와 함께 설명해서 자연어 과제 중 가장 수월하게 해낼 수 있었다.  \n",
    "아쉽게도 maxlen을 15로 했음에도 data의 크기가 120000개가 넘었다. 같은 조인 상민님의 코드를 보면서 차후에 더 보완하겠다.  \n",
    "batch size는 메모리를 적게 쓰기 위해 작게 돌리면 생각보다 loss 가 많이 올라갔다. 32, 64, 256 중에 256 이 loss 가 가장 낮게 나왔다.   \n",
    "Embedding Size = 256이 가장 적당했다.  \n",
    "***\n",
    "## 한 걸음 더 \n",
    "Hidden Size = 1024 일때는 로스가 2.2보다 살짝 적었지만, 직접 가사를 써보면 자연스러운 문장이 나왔다.  \n",
    "'baby'는 팝송에서 후렴으로 자주 불러서 반복적인 가사가 나온것 같다.  \n",
    "  \n",
    "loss를 줄이기 위해 hidden size를 2배로 늘렸더니, 실행 시간은 시간이 많이 늘어났고 예상대로 loss는 낮아졌다.  \n",
    "그런데 실제로 가사를 뽑아보니다. 한 단어를 너무 많이 반복하는 경우가 생겼고, 이는 부자연스럽다. 즉, overfitting 되었다고 추측할 수 있는 결과가 나왔다.  \n",
    "  \n",
    "이로써 loss가 작은 것도 중요하지만, 적당한 hidden size를 찾는 것이 중요함을 알았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-transcription",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
